'use client';

import { useEffect, useRef } from 'react';
import { FaceLandmarker, FilesetResolver } from '@mediapipe/tasks-vision';
import { 
    calculateEAR, 
    getDistance, 
    getIrisPosition, 
    calculateNostrilDilatation,
    calculateStdDev,
    determineMicroExpression
    } from '../utils/faceMath';

    interface FaceDataPayload {
    roomId: string;
    deviceId: string;
    data: {
        eyeBlinkRate: number;    // ëˆˆ ê¹œë¹¡ì„ (íšŒ/ì´ˆ)
        eyeMovement: number;     // ì‹œì„  ë¶ˆì•ˆì • (í‘œì¤€í¸ì°¨)
        facialTremor: number;    // ì–¼êµ´ ë–¨ë¦¼ (í‰ê·  ì´ë™ëŸ‰)
        nostrilMovement: number; // ì½§êµ¬ë© ì›€ì§ì„ (ë³€í™”ëŸ‰)
        microExpression: string; // "nervous" | "neutral" | "confident"
    }
    }

    export default function FaceTracker({ roomId, onStatusChange }: { roomId: string, onStatusChange?: (msg: string) => void }) {
    const videoRef = useRef<HTMLVideoElement>(null);
    const lastTimeRef = useRef<number>(-1);
    const faceLandmarkerRef = useRef<FaceLandmarker | null>(null);
    
    // 1ì´ˆ ë™ì•ˆ ë°ì´í„°ë¥¼ ìŒ“ì•„ë‘˜ ì°½ê³  (í†µê³„ìš©)
    const statsRef = useRef({
        blinks: 0,
        isEyeClosed: false,
        
        // ë°ì´í„° ëª¨ìŒì§‘ (1ì´ˆ ë’¤ì— ê³„ì‚°í•˜ê³  ë¹„ìš¸ ê±°ì„)
        irisMovements: [] as number[],   // ëˆˆë™ì ì´ë™ ê±°ë¦¬ë“¤
        headMovements: [] as number[],   // ë¨¸ë¦¬ ì´ë™ ê±°ë¦¬ë“¤ (ë–¨ë¦¼)
        nostrilSizes: [] as number[],    // ì½§êµ¬ë© í¬ê¸°ë“¤
        
        lastIris: null as any,
        lastNose: null as any,
    });

    useEffect(() => {
        const setupFaceMesh = async () => {
        onStatusChange?.("AI ëª¨ë¸ ë¡œë”© ì¤‘...");
        const vision = await FilesetResolver.forVisionTasks(
            "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
        );
        
        faceLandmarkerRef.current = await FaceLandmarker.createFromOptions(vision, {
            baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
            delegate: "GPU"
            },
            outputFaceBlendshapes: true,
            runningMode: "VIDEO",
            numFaces: 1
        });
        startCamera();
        };
        setupFaceMesh();
    }, []);

    const startCamera = async () => {
        if (!videoRef.current) return;
        try {
        const stream = await navigator.mediaDevices.getUserMedia({ 
            video: { width: 480, height: 480, facingMode: "user" } 
        });
        videoRef.current.srcObject = stream;
        videoRef.current.addEventListener("loadeddata", predictWebcam);
        onStatusChange?.("ì§„ì‹¤ì˜ ëˆˆ ê°€ë™ ì¤‘... ğŸ‘ï¸");
        } catch (err) {
        console.error(err);
        onStatusChange?.("ì¹´ë©”ë¼ ê¶Œí•œ í•„ìš” ğŸ“·");
        }
    };

    const predictWebcam = () => {
        const video = videoRef.current;
        const landmarker = faceLandmarkerRef.current;
        
        if (video && landmarker) {
        let startTimeMs = performance.now();
        
        if (lastTimeRef.current !== video.currentTime) {
            lastTimeRef.current = video.currentTime;
            const results = landmarker.detectForVideo(video, startTimeMs);

            if (results.faceLandmarks && results.faceLandmarks.length > 0) {
            const landmarks = results.faceLandmarks[0];
            const stats = statsRef.current;

            // 1. ëˆˆ ê¹œë¹¡ì„ (Count)
            const ear = calculateEAR(landmarks);
            if (ear < 0.25) {
                if (!stats.isEyeClosed) {
                    stats.blinks += 1;
                    stats.isEyeClosed = true;
                }
            } else {
                stats.isEyeClosed = false;
            }

            // 2. ì‹œì„  ë¶ˆì•ˆì • (Iris Movement)
            const currentIris = getIrisPosition(landmarks);
            if (stats.lastIris) {
                // ì™¼ìª½ ëˆˆë™ìê°€ ì›€ì§ì¸ ê±°ë¦¬
                const dist = getDistance(currentIris.left, stats.lastIris.left);
                stats.irisMovements.push(dist);
            }
            stats.lastIris = currentIris;

            // 3. ì–¼êµ´ ë–¨ë¦¼ (Head Tremor - ì½” ë ê¸°ì¤€)
            const currentNose = landmarks[1];
            if (stats.lastNose) {
                const dist = getDistance(currentNose, stats.lastNose);
                stats.headMovements.push(dist);
            }
            stats.lastNose = currentNose;

            // 4. ì½§êµ¬ë© ì›€ì§ì„ (Nostril)
            const currentNostrilSize = calculateNostrilDilatation(landmarks);
            stats.nostrilSizes.push(currentNostrilSize);
            }
        }
        requestAnimationFrame(predictWebcam);
        }
    };

    // 1ì´ˆë§ˆë‹¤ ë°±ì—”ë“œë¡œ ë¶„ì„ ê²°ê³¼ ì „ì†¡ 
    useEffect(() => {
        const interval = setInterval(async () => {
            const stats = statsRef.current;
            
            // ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ íŒ¨ìŠ¤
            if (stats.headMovements.length < 5) return;

            // í†µê³„ ê³„ì‚° & 0~1 ì ìˆ˜ ë³€í™˜
            
            // 1. ì‹œì„  ë¶ˆì•ˆì • (í‘œì¤€í¸ì°¨)
            const rawEyeMovement = calculateStdDev(stats.irisMovements);
            const eyeMovementScore = Math.min(rawEyeMovement * 50, 1.0); 

            // 2. ì–¼êµ´ ë–¨ë¦¼ (í‰ê·  ì´ë™ëŸ‰)
            const rawTremor = stats.headMovements.reduce((a,b)=>a+b, 0) / stats.headMovements.length;
            const tremorScore = Math.min(rawTremor * 30, 1.0);

            // 3. ì½§êµ¬ë© ì›€ì§ì„ (í‘œì¤€í¸ì°¨)
            const rawNostril = calculateStdDev(stats.nostrilSizes);
            const nostrilScore = Math.min(rawNostril * 100, 1.0);

            // 4. ë¯¸ì„¸í‘œì • íŒì • (ë³€í™˜ëœ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨)
            const expression = determineMicroExpression(stats.blinks, eyeMovementScore, tremorScore);

            // ì „ì†¡í•  ë°ì´í„° íŒ¨í‚¤ì§€
            const payload: FaceDataPayload = {
                roomId: roomId,
                deviceId: "player_device_id", 
                data: {
                    eyeBlinkRate: stats.blinks, 
                    
                    eyeMovement: parseFloat(eyeMovementScore.toFixed(2)), 
                    facialTremor: parseFloat(tremorScore.toFixed(2)),
                    nostrilMovement: parseFloat(nostrilScore.toFixed(2)),
                    
                    microExpression: expression
                }
            };

            // ì „ì†¡
            try {
                await fetch('/api/v1/games/truth/face-tracking', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });
                // ë¡œê·¸ë¡œ ì ìˆ˜ í™•ì¸í•´ë³´ê¸°!
                console.log(`ì „ì†¡: ëˆˆ(${payload.data.eyeMovement}) ë–¨ë¦¼(${payload.data.facialTremor}) ì½”(${payload.data.nostrilMovement})`);
                onStatusChange?.(`ë¶„ì„ ì¤‘... ìƒíƒœ: ${expression}`);
            } catch (e) {
                console.error("ì „ì†¡ ì‹¤íŒ¨");
            }

            // ì´ˆê¸°í™”
            stats.blinks = 0;
            stats.irisMovements = [];
            stats.headMovements = [];
            stats.nostrilSizes = [];

        }, 1000); 

        return () => clearInterval(interval);
    }, [roomId]);

    return (
        <div className="relative w-full max-w-[300px] aspect-square bg-gray-900 rounded-2xl overflow-hidden border-2 border-purple-500 shadow-lg mx-auto">
        <video 
            ref={videoRef} 
            autoPlay 
            playsInline
            muted
            className="w-full h-full object-cover transform scale-x-[-1]" 
        />
        <div className="absolute top-2 left-2 bg-black/60 text-xs px-2 py-1 rounded-md text-green-400 border border-green-500/30 animate-pulse">
            â— LIVE ANALYSIS
        </div>
        </div>
    );
}