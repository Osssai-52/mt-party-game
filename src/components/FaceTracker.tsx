'use client';

import { useEffect, useRef } from 'react';
import { FaceLandmarker, FilesetResolver } from '@mediapipe/tasks-vision';
import gameApi from '../services/gameApi';
import { 
    calculateEAR, 
    getDistance, 
    getIrisPosition, 
    calculateNostrilDilatation,
    calculateStdDev,
    determineMicroExpression
} from '../utils/faceMath';
import { FaceAnalysisData } from '../types/truth';

interface FaceTrackerProps {
    roomId: string;
    targetDeviceId: string; // í˜„ì¬ ë‹µë³€ìì˜ deviceId (ë°ì´í„° ì „ì†¡ìš©)
    onStatusChange?: (msg: string) => void;
    onAnalyze?: (data: FaceAnalysisData) => void; // âœ¨ ë¶€ëª¨ì—ê²Œ ë°ì´í„° ì „ë‹¬ìš© ì½œë°±
}

export default function FaceTracker({ roomId, targetDeviceId, onStatusChange, onAnalyze }: FaceTrackerProps) {
    const videoRef = useRef<HTMLVideoElement>(null);
    const lastTimeRef = useRef<number>(-1);
    const faceLandmarkerRef = useRef<FaceLandmarker | null>(null);
    
    // 1ì´ˆ ë™ì•ˆ ë°ì´í„°ë¥¼ ìŒ“ì•„ë‘˜ í†µê³„ ì°½ê³ 
    const statsRef = useRef({
        blinks: 0,
        isEyeClosed: false,
        irisMovements: [] as number[],
        headMovements: [] as number[],
        nostrilSizes: [] as number[],
        lastIris: null as any,
        lastNose: null as any,
    });

    useEffect(() => {
        const setupFaceMesh = async () => {
            onStatusChange?.("AI ëª¨ë¸ ë¡œë”© ì¤‘...");
            const vision = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
            );
            
            faceLandmarkerRef.current = await FaceLandmarker.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
                    delegate: "GPU"
                },
                outputFaceBlendshapes: true,
                runningMode: "VIDEO",
                numFaces: 1
            });
            startCamera();
        };
        setupFaceMesh();
    }, []);

    const startCamera = async () => {
        if (!videoRef.current) return;
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ 
                video: { width: 640, height: 480, facingMode: "user" } 
            });
            videoRef.current.srcObject = stream;
            videoRef.current.addEventListener("loadeddata", predictWebcam);
            onStatusChange?.("ì§„ì‹¤ì˜ ëˆˆ ê°€ë™ ì¤‘... ğŸ‘ï¸");
        } catch (err) {
            console.error(err);
            onStatusChange?.("ì¹´ë©”ë¼ ê¶Œí•œ í•„ìš” ğŸ“·");
        }
    };

    const predictWebcam = () => {
        const video = videoRef.current;
        const landmarker = faceLandmarkerRef.current;
        
        if (video && landmarker) {
            let startTimeMs = performance.now();
            if (lastTimeRef.current !== video.currentTime) {
                lastTimeRef.current = video.currentTime;
                const results = landmarker.detectForVideo(video, startTimeMs);

                if (results.faceLandmarks && results.faceLandmarks.length > 0) {
                    const landmarks = results.faceLandmarks[0];
                    const stats = statsRef.current;

                    // 1. ëˆˆ ê¹œë¹¡ì„ (Count)
                    const ear = calculateEAR(landmarks);
                    if (ear < 0.25) {
                        if (!stats.isEyeClosed) {
                            stats.blinks += 1;
                            stats.isEyeClosed = true;
                        }
                    } else {
                        stats.isEyeClosed = false;
                    }

                    // 2. ì‹œì„  ë¶ˆì•ˆì • (Iris Movement)
                    const currentIris = getIrisPosition(landmarks);
                    if (stats.lastIris) {
                        const dist = getDistance(currentIris.left, stats.lastIris.left);
                        stats.irisMovements.push(dist);
                    }
                    stats.lastIris = currentIris;

                    // 3. ì–¼êµ´ ë–¨ë¦¼ (Head Tremor - ì½” ë ê¸°ì¤€)
                    const currentNose = landmarks[1];
                    if (stats.lastNose) {
                        const dist = getDistance(currentNose, stats.lastNose);
                        stats.headMovements.push(dist);
                    }
                    stats.lastNose = currentNose;

                    // 4. ì½§êµ¬ë© ì›€ì§ì„ (Nostril)
                    const currentNostrilSize = calculateNostrilDilatation(landmarks);
                    stats.nostrilSizes.push(currentNostrilSize);
                }
            }
            requestAnimationFrame(predictWebcam);
        }
    };

    // â±ï¸ 1ì´ˆë§ˆë‹¤ ë°ì´í„° ë¶„ì„ ë° ì „ì†¡
    useEffect(() => {
        const interval = setInterval(async () => {
            const stats = statsRef.current;
            
            // ë°ì´í„°ê°€ ì¶©ë¶„ì¹˜ ì•Šìœ¼ë©´ íŒ¨ìŠ¤
            if (stats.headMovements.length < 5) return;

            // --- ì ìˆ˜ ë³€í™˜ ë¡œì§ ---
            const eyeMovementScore = Math.min(calculateStdDev(stats.irisMovements) * 50, 1.0); 
            const tremorScore = Math.min((stats.headMovements.reduce((a,b)=>a+b, 0) / stats.headMovements.length) * 30, 1.0);
            const nostrilScore = Math.min(calculateStdDev(stats.nostrilSizes) * 100, 1.0);
            
            // ì¢…í•© ìŠ¤íŠ¸ë ˆìŠ¤ ì§€ìˆ˜ (0 ~ 100)
            const stressLevel = Math.min(
                (stats.blinks * 10) + (eyeMovementScore * 40) + (tremorScore * 30) + (nostrilScore * 20), 
                100
            );

            const isLie = stressLevel >= 55; // íŒì • ê¸°ì¤€
            
            // ë°ì´í„° íŒ¨í‚¤ì§•
            const analysisData: FaceAnalysisData = {
                eyeBlinkRate: stats.blinks,
                eyeMovement: parseFloat(eyeMovementScore.toFixed(2)),
                facialTremor: parseFloat(tremorScore.toFixed(2)),
                nostrilMovement: parseFloat(nostrilScore.toFixed(2)),
                stressLevel: Math.round(stressLevel),
                isLie
            };

            // 1. ğŸ“¡ ì„œë²„ë¡œ ì „ì†¡ (ê²Œì„ API ì‚¬ìš©)
            try {
                // targetDeviceIdëŠ” ì§€ê¸ˆ ë‹µë³€í•˜ê³  ìˆëŠ” ì‚¬ëŒì˜ IDì—¬ì•¼ í•¨
                if (targetDeviceId) {
                    await gameApi.truth.sendFaceData(roomId, targetDeviceId, analysisData);
                    console.log(`[FaceTracker] ì „ì†¡ ì™„ë£Œ: Stress ${stressLevel}`);
                }
            } catch (e) { console.error("ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨", e); }

            // 2. âœ¨ ë¶€ëª¨(HUD) í™”ë©´ ì—…ë°ì´íŠ¸
            if (onAnalyze) {
                onAnalyze(analysisData);
            }

            // ì´ˆê¸°í™”
            stats.blinks = 0;
            stats.irisMovements = [];
            stats.headMovements = [];
            stats.nostrilSizes = [];

        }, 1000); 

        return () => clearInterval(interval);
    }, [roomId, targetDeviceId, onAnalyze]);

    return (
        // FaceTrackerëŠ” ì´ì œ ë°°ê²½ìœ¼ë¡œ ì“°ì¼ ê±°ë¼ ìŠ¤íƒ€ì¼ì„ ì¡°ê¸ˆ ìˆ˜ì •í–ˆì–´
        <div className="w-full h-full">
            <video 
                ref={videoRef} 
                autoPlay 
                playsInline
                muted
                className="w-full h-full object-cover transform scale-x-[-1]" 
            />
        </div>
    );
}